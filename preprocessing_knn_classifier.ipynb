{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a7fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler,OneHotEncoder\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b62fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "base_df = pd.read_csv('atp_transformed/2000-2024 players_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841b314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = base_df\n",
    "len(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "# 'surface', \n",
    "'tourney_level', \n",
    "# 'tourney_date', # dropped because not numeric \n",
    "'match_num', \n",
    "#'player_height', \n",
    "# 'player_country', \n",
    "'player_age', \n",
    "# 'elo_pre_match', # dropped because of colinearity\n",
    "# 'opponent_elo_pre_match', \n",
    "#'opponent_rank', \n",
    "# 'mean_numb', # dropped as the rolling means are derrivitives of these columns and adding the raw ones creates a worse model and increases conlinearity\n",
    "# 'median_numb', \n",
    "# 'total_numb', \n",
    "# 'mean_diff', \n",
    "# 'median_diff', \n",
    "# 'total_diff', \n",
    "# 'mean_tb_numb', \n",
    "# 'median_tb_numb', \n",
    "# 'total_tb_numb', \n",
    "# 'mean_tb_diff', \n",
    "# 'median_tb_diff', \n",
    "# 'total_tb_diff', \n",
    "'days_of_experience',\n",
    "# 'career_year', \n",
    "# 'rest_days', \n",
    "# 'set_dominance', \n",
    "# 'tb_dominance', \n",
    "#'highest_finish_position', \n",
    "'draw_size_rolling_med_10', \n",
    "'highest_finish_position_rolling_med_10', \n",
    "# 'ace_rolling_mean_10', \n",
    "# 'double_faults_rolling_mean_10', # drop these so that most low level players are included\n",
    "# 'points_on_serve_rolling_mean_10', \n",
    "# 'first_serve_in_rolling_mean_10', \n",
    "# '1stWon_rolling_mean_10', \n",
    "# '2ndWon_rolling_mean_10', \n",
    "# 'service_games_rolling_mean_10', \n",
    "# 'break_points_saved_rolling_mean_10', \n",
    "# 'break_points_faced_rolling_mean_10', \n",
    "'elo_pre_match_rolling_mean_10',\n",
    "'opponent_elo_pre_match_rolling_mean_10', \n",
    "'set_dominance_rolling_mean_10', \n",
    "'tb_dominance_rolling_mean_10', \n",
    "#'player_rank_rolling_mean_10', # dropped because the model will cheat with this if predicting rank\n",
    "'mean_numb_rolling_mean_10', \n",
    "#'median_numb_rolling_mean_10', \n",
    "#'total_numb_rolling_mean_10', \n",
    "'mean_diff_rolling_mean_10', \n",
    "#'median_diff_rolling_mean_10', \n",
    "#'total_diff_rolling_mean_10', \n",
    "'mean_tb_numb_rolling_mean_10', \n",
    "#'median_tb_numb_rolling_mean_10', \n",
    "#'total_tb_numb_rolling_mean_10', \n",
    "'mean_tb_diff_rolling_mean_10', \n",
    "#'median_tb_diff_rolling_mean_10', \n",
    "#'total_tb_diff_rolling_mean_10', \n",
    "# 'elo_next_match',\n",
    "'binned_rank' # target\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cdbc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[['median_tb_diff','mean_tb_numb','median_tb_numb','mean_tb_diff','tb_dominance']] = full_df[['median_tb_diff','mean_tb_numb','median_tb_numb','mean_tb_diff','tb_dominance']].replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b5980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin the data using 1-25, 26-100, 101-300, 301-600, 601-1000, 1001-2268 bins as they represent the strength of players in a better way\n",
    "\n",
    "bins = [1, 25, 100, 300, 600, 1000, 2268]\n",
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "full_df['binned_rank'] = pd.cut(\n",
    "    full_df['filled_player_rank'], \n",
    "    bins=bins, \n",
    "    labels=labels, \n",
    "    include_lowest=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8d4120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop all nan values\n",
    "# full_df = full_df.dropna(subset=['player_rank'])\n",
    "\n",
    "# # log scale the rank\n",
    "# full_df['log_rank'] = np.log(full_df['player_rank'])\n",
    "\n",
    "# num_bins = 20\n",
    "\n",
    "# # Bin the ranks\n",
    "# full_df['binned_rank'] = pd.qcut(full_df['player_rank'], q=num_bins, labels=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d8fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(full_df))\n",
    "df_subset = full_df[selected_features]\n",
    "print(len(df_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d37a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all features to use in the prediction\n",
    "df_subset = full_df[selected_features]\n",
    "\n",
    "df_subset.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3dc428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all values where nan because knn cannot deal with empty values\n",
    "df_subset = df_subset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd25dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4707b7f3",
   "metadata": {},
   "source": [
    "From testing the categorical columns add no value to the prediction. Remoiving them creates a better model for all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8defa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print non numeric columns\n",
    "print(len(df_subset))\n",
    "numeric_cols = df_subset.select_dtypes(exclude=[np.number]).columns\n",
    "print(numeric_cols)\n",
    "\n",
    "# encode non numeric values\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop first to avoid multicollinearity\n",
    "surface_encoded = encoder.fit_transform(df_subset[[\n",
    "    # 'surface',\n",
    "    #'player_country',\n",
    "    'tourney_level'\n",
    "    ]])\n",
    "surface_df = pd.DataFrame(surface_encoded, \n",
    "                          columns=encoder.get_feature_names_out([\n",
    "    #'surface',\n",
    "    #'player_country',\n",
    "    'tourney_level'\n",
    "                                                                 ]),\n",
    "                          index=df_subset.index)\n",
    "df_encoded = pd.concat([df_subset.drop([\n",
    "    #'surface',\n",
    "    #'player_country',\n",
    "    'tourney_level'\n",
    "    ], axis=1), surface_df], axis=1)\n",
    "#df_encoded = df_subset\n",
    "print(len(df_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86677fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "X = df_encoded.drop('binned_rank', axis=1)  # Features\n",
    "y = df_encoded['binned_rank']  # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb30026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2db666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features to normalise\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select K and weights based on library\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "X_sub, _, y_sub, _ = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    train_size=100_000,   # even 50k often sufficient\n",
    "    stratify=y_train,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'knn__n_neighbors': list(range(1, 35))\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=3, return_train_score=True,verbose=2,n_jobs=-1)\n",
    "grid.fit(X_sub, y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b674ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract k values\n",
    "k_values = param_grid['knn__n_neighbors']\n",
    "\n",
    "# Extract mean test scores and standard deviation from CV\n",
    "mean_test_scores = grid.cv_results_['mean_test_score']\n",
    "std_test_scores = grid.cv_results_['std_test_score']\n",
    "mean_train_scores = grid.cv_results_['mean_train_score']\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(k_values, mean_train_scores, label='Train Accuracy', marker='o',color='orangered')\n",
    "plt.plot(k_values, mean_test_scores, label='CV Accuracy', marker='o',color='seagreen')\n",
    "plt.xlabel(\"Number of Neighbors (k)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"KNN Train vs CV Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d068d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "knn = KNeighborsClassifier(n_neighbors=19,weights='uniform')\n",
    "knn.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred = knn.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c6409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Check if there's overfitting\n",
    "train_score = knn.score(X_train_scaled, y_train)\n",
    "test_score = knn.score(X_test_scaled, y_test)\n",
    "print(f\"Train accuracy: {train_score:.4f}\")\n",
    "print(f\"Test accuracy: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5020391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_labels = [f\"{bins[i]}-{bins[i+1]}\" for i in range(len(bins)-1)]\n",
    "\n",
    "#bin_edges = pd.qcut(full_df['player_rank'], q=num_bins).cat.categories\n",
    "\n",
    "# Map numeric predictions to readable strings\n",
    "#bin_labels = [f\"{round(interval.left,2)}-{round(interval.right,2)}\" for interval in bin_edges]\n",
    "\n",
    "y_pred_str = [bin_labels[i] for i in y_pred]\n",
    "y_test_str = [bin_labels[j] for j in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaacf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.Series(y_pred)\n",
    "results = X_test.reset_index(drop=True).copy()\n",
    "results['true_rank_coded'] = y_test.reset_index(drop=True)\n",
    "results['pred_rank_coded'] = y_pred\n",
    "results['true_rank_coded'] = results['true_rank_coded'].astype(int)\n",
    "results['pred_rank_coded'] = results['pred_rank_coded'].astype(int)\n",
    "results['true_rank'] = y_test_str\n",
    "results['pred_rank'] = y_pred_str\n",
    "results['correct'] = results['true_rank'] == results['pred_rank']\n",
    "results['number_of_bins_off'] = results['true_rank_coded'] - results['pred_rank_coded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_preds = results[results['correct'] == False]\n",
    "\n",
    "len(wrong_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b10d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = wrong_preds['number_of_bins_off'].value_counts().sort_index()/len(wrong_preds)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.xlabel(\"Number of Bins Off\")\n",
    "plt.ylabel(\"Percentage of Errors\")\n",
    "plt.title(\"Distribution of Prediction Errors (using 6 bins and k=19\")\n",
    "plt.tight_layout()\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1.0))\n",
    "bars = plt.bar(counts.index.astype(str), counts.values)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        height,\n",
    "        f\"{height:.1%}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\"\n",
    "    )\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of pred player_ranks and true player_ranks\n",
    "pred_counts = wrong_preds['pred_rank_coded'].value_counts()\n",
    "true_counts = wrong_preds['true_rank_coded'].value_counts()\n",
    "\n",
    "x = np.arange(len(pred_counts))  # the x locations\n",
    "width = 0.35  # width of the bars\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(x - width/2, pred_counts.values, width=width, label='Predicted')\n",
    "plt.bar(x + width/2, true_counts.values, width=width, label='True')\n",
    "\n",
    "plt.xlabel(\"Player Rank\")\n",
    "plt.ylabel(\"Count of Samples\")\n",
    "plt.title(\"Distribution of Predicted vs True Population (6 bins, k=19)\")\n",
    "plt.xticks(x, bin_labels)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b5a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each binned rank\n",
    "pred_counts = results['pred_rank_coded'].value_counts() / len(results)\n",
    "\n",
    "x = np.arange(len(pred_counts))  # x positions\n",
    "width = 0.8\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(x, pred_counts.values, width=width)\n",
    "\n",
    "plt.xlabel(\"Player bins\")\n",
    "plt.ylabel(\"Total rows\")\n",
    "plt.title(\"Total dataset population\")\n",
    "\n",
    "# Use your string bin labels on the x-axis\n",
    "plt.xticks(x, bin_labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0adfafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each binned rank\n",
    "counts = df_encoded['binned_rank'].value_counts().sort_index() / len(df_encoded)\n",
    "\n",
    "x = np.arange(len(counts))  # x positions\n",
    "width = 0.8\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(x, counts.values, width=width)\n",
    "\n",
    "plt.xlabel(\"Player bins\")\n",
    "plt.ylabel(\"Total rows\")\n",
    "plt.title(\"Total dataset population\")\n",
    "\n",
    "# Use your string bin labels on the x-axis\n",
    "plt.xticks(x, bin_labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b622b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True distribution\n",
    "counts = df_encoded['binned_rank'].value_counts().sort_index() / len(df_encoded)\n",
    "\n",
    "# Predicted distribution\n",
    "pred_counts = results['pred_rank_coded'].value_counts().sort_index() / len(results)\n",
    "\n",
    "x = np.arange(len(counts))  # x positions\n",
    "width = 0.35  # width of bars\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "bars_true = plt.bar(x - width/2, counts.values, width=width, label='Real Population', color='seagreen')\n",
    "bars_pred = plt.bar(x + width/2, pred_counts.values, width=width, label='Predicted Population', color='orangered')\n",
    "\n",
    "plt.xlabel(\"Player bins\")\n",
    "plt.ylabel(\"Proportion of Total\")\n",
    "plt.title(\"True vs Predicted Player Bin Distribution\")\n",
    "plt.xticks(x, bin_labels)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add labels on top of the bars\n",
    "for bar in bars_true:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height, f\"{height:.2f}\", ha='center', va='bottom')\n",
    "\n",
    "for bar in bars_pred:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height, f\"{height:.2f}\", ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3abd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",xticklabels=bin_labels,yticklabels=bin_labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e02fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
