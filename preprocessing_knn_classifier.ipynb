{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a7fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b62fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "full_df = pd.read_csv('atp_transformed/2000-2024 players_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['surface', \n",
    "'tourney_level', \n",
    "#'tourney_date', dropped because not numeric \n",
    "'match_num', \n",
    "'player_seed', \n",
    "'player_height', \n",
    "'player_country', \n",
    "'player_age', \n",
    "#'elo_pre_match', dropped because of leakage\n",
    "#'opponent_elo_pre_match', \n",
    "'opponent_rank', \n",
    "'mean_numb', \n",
    "'median_numb', \n",
    "'total_numb', \n",
    "'mean_diff', \n",
    "'median_diff', \n",
    "'total_diff', \n",
    "'mean_tb_numb', \n",
    "'median_tb_numb', \n",
    "'total_tb_numb', \n",
    "'mean_tb_diff', \n",
    "'median_tb_diff', \n",
    "'total_tb_diff', \n",
    "'days_of_experience',\n",
    "'career_year', \n",
    "'rest_days', \n",
    "'set_dominance', \n",
    "'tb_dominance', \n",
    "'highest_finish_position', \n",
    "'minutes_rolling_med_10', \n",
    "'draw_size_rolling_med_10', \n",
    "'highest_finish_position_rolling_med_10', \n",
    "'ace_rolling_mean_10', \n",
    "'double_faults_rolling_mean_10', \n",
    "'points_on_serve_rolling_mean_10', \n",
    "'first_serve_in_rolling_mean_10', \n",
    "'1stWon_rolling_mean_10', \n",
    "'2ndWon_rolling_mean_10', \n",
    "'service_games_rolling_mean_10', \n",
    "'break_points_saved_rolling_mean_10', \n",
    "'break_points_faced_rolling_mean_10', \n",
    "# 'elo_pre_match_rolling_mean_10', dropped because of leakage\n",
    "# 'opponent_elo_pre_match_rolling_mean_10', \n",
    "'set_dominance_rolling_mean_10', \n",
    "'tb_dominance_rolling_mean_10', \n",
    "'player_rank_rolling_mean_10', \n",
    "'mean_numb_rolling_mean_10', \n",
    "'median_numb_rolling_mean_10', \n",
    "'total_numb_rolling_mean_10', \n",
    "'mean_diff_rolling_mean_10', \n",
    "'median_diff_rolling_mean_10', \n",
    "'total_diff_rolling_mean_10', \n",
    "'mean_tb_numb_rolling_mean_10', \n",
    "'median_tb_numb_rolling_mean_10', \n",
    "'total_tb_numb_rolling_mean_10', \n",
    "'mean_tb_diff_rolling_mean_10', \n",
    "'median_tb_diff_rolling_mean_10', \n",
    "'total_tb_diff_rolling_mean_10', \n",
    "'elo_next_match',\n",
    "'binned_rank' # target\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8d4120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all nan values\n",
    "full_df['player_rank'] = full_df['player_rank'].dropna()\n",
    "\n",
    "num_bins = 20\n",
    "\n",
    "# Bin the ranks\n",
    "full_df['binned_rank'] = pd.qcut(full_df['player_rank'], q=num_bins, labels=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d37a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all features to use in the prediction\n",
    "df_subset = full_df[selected_features]\n",
    "\n",
    "# drop all values where nan because knn cannot deal with empty values\n",
    "df_subset = df_subset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8defa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print non numeric columns\n",
    "#numeric_cols = df_subset.select_dtypes(exclude=[np.number]).columns\n",
    "#print(numeric_cols)\n",
    "\n",
    "# encode non numeric values\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop first to avoid multicollinearity\n",
    "surface_encoded = encoder.fit_transform(df_subset[['surface','player_country','tourney_level']])\n",
    "surface_df = pd.DataFrame(surface_encoded, \n",
    "                          columns=encoder.get_feature_names_out(['surface','player_country','tourney_level']),\n",
    "                          index=df_subset.index)\n",
    "df_encoded = pd.concat([df_subset.drop(['surface','player_country','tourney_level'], axis=1), surface_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86677fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "X = df_encoded.drop('binned_rank', axis=1)  # Features\n",
    "y = df_encoded['binned_rank']  # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb30026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2db666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features to normalise\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "knn = KNeighborsClassifier(n_neighbors=2) # using trial and error 2 seemed to be the best score but it's still bad\n",
    "knn.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d068d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code says that k=1 is the best k, but k=1 is not going to work on other models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'knn__n_neighbors': list(range(1, 50))\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred = knn.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c6409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Check if there's overfitting\n",
    "train_score = knn.score(X_train_scaled, y_train)\n",
    "test_score = knn.score(X_test_scaled, y_test)\n",
    "print(f\"Train accuracy: {train_score:.4f}\")\n",
    "print(f\"Test accuracy: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5020391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = pd.qcut(full_df['player_rank'], q=num_bins).cat.categories\n",
    "\n",
    "# Map numeric predictions to readable strings\n",
    "bin_labels = [f\"{round(interval.left,2)}-{round(interval.right,2)}\" for interval in bin_edges]\n",
    "y_pred_str = [bin_labels[i] for i in y_pred]\n",
    "y_test_str = [bin_labels[j] for j in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaacf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = X_test.reset_index(drop=True).copy()\n",
    "results['true_rank_coded'] = y_test.reset_index(drop=True)\n",
    "results['pred_rank_coded'] = y_pred\n",
    "results['true_rank'] = y_test_str\n",
    "results['pred_rank'] = y_pred_str\n",
    "results['correct'] = results['true_rank'] == results['pred_rank']\n",
    "results['number_of_bins_off'] = results['true_rank_coded'] - results['pred_rank_coded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_preds = results[results['correct'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b10d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = wrong_preds['number_of_bins_off'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(counts.index.astype(str), counts.values)\n",
    "plt.xlabel(\"Number of Bins Off\")\n",
    "plt.ylabel(\"Count of Samples\")\n",
    "plt.title(\"Distribution of Prediction Errors (using bins=20 and k=2)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3abd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e02fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
